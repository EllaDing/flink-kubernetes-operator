Class Name,Config Option,Description
BatchExecutionOptions,execution.batch.adaptive.auto-parallelism.enabled,"If true, Flink will automatically decide the parallelism of operators in batch jobs."
BatchExecutionOptions,execution.batch.adaptive.auto-parallelism.min-parallelism,"The lower bound of allowed parallelism to set adaptively if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>"
BatchExecutionOptions,execution.batch.adaptive.auto-parallelism.max-parallelism,"The upper bound of allowed parallelism to set adaptively if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>"
BatchExecutionOptions,execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task,"The average size of data volume to expect each task instance to process if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>. Note that when data skew occurs or the decided parallelism reaches the <code class=""highlighter-rouge"">execution.batch.adaptive.auto-parallelism.max-parallelism</code> (due to too much data), the data actually processed by some tasks may far exceed this value."
BatchExecutionOptions,execution.batch.adaptive.auto-parallelism.default-source-parallelism,"The default parallelism of source vertices or the upper bound of source parallelism to set adaptively if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>. Note that <code class=""highlighter-rouge"">execution.batch.adaptive.auto-parallelism.max-parallelism</code> will be used if this configuration is not configured. If <code class=""highlighter-rouge"">execution.batch.adaptive.auto-parallelism.max-parallelism</code> is not set either, then the default parallelism set via <code class=""highlighter-rouge"">parallelism.default</code> will be used instead."
BatchExecutionOptions,execution.batch.speculative.enabled,Controls whether to enable speculative execution.
BatchExecutionOptions,execution.batch.speculative.max-concurrent-executions,"Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones."
BatchExecutionOptions,execution.batch.speculative.block-slow-node-duration,Controls how long an detected slow node should be blocked for.
BatchExecutionOptions,execution.batch.job-recovery.enabled,"A flag to enable or disable the job recovery. If enabled, batch jobs can resume with previously generated intermediate results after job master restarts due to failures, thereby preserving the progress."
BatchExecutionOptions,execution.batch.job-recovery.previous-worker.recovery.timeout,"The timeout for a new job master to wait for the previous worker to reconnect.A reconnected worker will transmit the details of its produced intermediate results to the new job master, enabling the job master to reuse these results."
BatchExecutionOptions,execution.batch.job-recovery.snapshot.min-pause,The minimal pause between snapshots taken by operator coordinator or other components. It is used to avoid performance degradation due to excessive snapshot frequency.
WebOptions,web.address,Address for runtime monitor web-frontend server.
WebOptions,web.port,
WebOptions,web.access-control-allow-origin,Access-Control-Allow-Origin header for all responses from the web-frontend.
WebOptions,web.refresh-interval,Refresh interval for the web-frontend.
WebOptions,web.ssl.enabled,Flag indicating whether to override SSL support for the JobManager Web UI.
WebOptions,web.tmpdir,Local directory that is used by the REST API for temporary files.
WebOptions,web.upload.dir,"Local directory that is used by the REST API for storing uploaded jars. If not specified a dynamic directory will be created under <code class=""highlighter-rouge"">web.tmpdir</code>."
WebOptions,web.history,Number of archived jobs for the JobManager.
WebOptions,web.log.path,Path to the log file (may be in /log for standalone but under log directory when using YARN).
WebOptions,web.submit.enable,Flag indicating whether jobs can be uploaded and run from the web-frontend.
WebOptions,web.cancel.enable,Flag indicating whether jobs can be canceled from the web-frontend.
WebOptions,web.rescale.enable,Flag indicating whether jobs can be rescaled from the web-frontend.
WebOptions,web.checkpoints.history,Number of checkpoints to remember for recent history.
WebOptions,web.exception-history-size,The maximum number of failures collected by the exception history per job.
WebOptions,web.backpressure.cleanup-interval,This config option is no longer used
WebOptions,web.backpressure.refresh-interval,This config option is no longer used
WebOptions,web.backpressure.num-samples,This config option is no longer used
WebOptions,web.backpressure.delay-between-samples,This config option is no longer used
WebOptions,web.timeout,Timeout for asynchronous operations by the web monitor.
AlgorithmOptions,taskmanager.runtime.hashjoin-bloom-filters,"Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles."
AlgorithmOptions,taskmanager.runtime.max-fan,"The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small."
AlgorithmOptions,taskmanager.runtime.sort-spilling-threshold,A sort operation starts spilling when this fraction of its memory budget is full.
AlgorithmOptions,taskmanager.runtime.large-record-handler,Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging.
DeploymentOptions,execution.target,"The deployment target for the execution. This can take one of the following values when calling <code class=""highlighter-rouge"">bin/flink run</code>:<ul><li>remote</li><li>local</li><li>yarn-per-job (deprecated)</li><li>yarn-session</li><li>kubernetes-session</li></ul>And one of the following values when calling <code class=""highlighter-rouge"">bin/flink run-application</code>:<ul><li>yarn-application</li><li>kubernetes-application</li></ul>"
DeploymentOptions,execution.attached,Specifies if the pipeline is submitted in attached or detached mode.
DeploymentOptions,execution.shutdown-on-attached-exit,"If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C."
DeploymentOptions,execution.job-listeners,Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments.
DeploymentOptions,execution.shutdown-on-application-finish,Whether a Flink Application cluster should shut down automatically after its application finishes (either successfully or as result of a failure). Has no effect for other deployment modes.
DeploymentOptions,execution.submit-failed-job-on-application-error,If a failed job should be submitted (in the application mode) when there is an error in the application driver before an actual job submission. This is intended for providing a clean way of reporting failures back to the user and is especially useful in combination with 'execution.shutdown-on-application-finish'. This option only works when the single job submission is enforced ('high-availability.type' is enabled). Please note that this is an experimental option and may be changed in the future.
DeploymentOptions,execution.program-config.wildcards,List of configuration keys that are allowed to be set in a user program regardless whether program configuration is enabled or not.<br /><br />Currently changes that are not backed by the Configuration class are always allowed.
DeploymentOptions,execution.program-config.enabled,"Determines whether configurations in the user program are allowed. By default, configuration can be set both on a cluster-level (via options) or within the user program (i.e. programmatic via environment setters). If disabled, all configuration must be defined on a cluster-level and programmatic setters in the user program are prohibited.<br /><br />Depending on your deployment mode failing the job might have different implications. Either your client that is trying to submit the job to an external cluster (session cluster deployment) throws the exception or the job manager (application mode deployment).<br /><br />The 'execution.program-config.wildcards' option lists configuration keys that are allowed to be set in user programs regardless of this setting."
CleanupOptions,cleanup-strategy.type,"Defines the cleanup strategy to use in case of cleanup failures.<br />Accepted values are:<ul><li><code class=""highlighter-rouge"">none</code>, <code class=""highlighter-rouge"">disable</code>, <code class=""highlighter-rouge"">off</code>: Cleanup is only performed once. No retry will be initiated in case of failure. The job artifacts (and the job's JobResultStore entry) have to be cleaned up manually in case of a failure.</li><li><code class=""highlighter-rouge"">fixed-delay</code>, <code class=""highlighter-rouge"">fixeddelay</code>: Cleanup attempts will be separated by a fixed interval up to the point where the cleanup is considered successful or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.</li><li><code class=""highlighter-rouge"">exponential-delay</code>, <code class=""highlighter-rouge"">exponentialdelay</code>: Exponential delay restart strategy triggers the cleanup with an exponentially increasing delay up to the point where the cleanup succeeded or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.</li></ul>The default configuration relies on an exponentially delayed retry strategy with the given default values."
CleanupOptions,cleanup-strategy.fixed-delay.attempts,"The number of times that Flink retries the cleanup before giving up if <code class=""highlighter-rouge"">cleanup-strategy.type</code> has been set to <code class=""highlighter-rouge"">fixed-delay</code>. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually."
CleanupOptions,cleanup-strategy.fixed-delay.delay,"Amount of time that Flink waits before re-triggering the cleanup after a failed attempt if the <code class=""highlighter-rouge"">cleanup-strategy.type</code> is set to <code class=""highlighter-rouge"">fixed-delay</code>. It can be specified using the following notation: ""1 min"", ""20 s"""
CleanupOptions,cleanup-strategy.exponential-delay.initial-backoff,"Starting duration between cleanup retries if <code class=""highlighter-rouge"">cleanup-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It can be specified using the following notation: ""1 min"", ""20 s"""
CleanupOptions,cleanup-strategy.exponential-delay.max-backoff,"The highest possible duration between cleanup retries if <code class=""highlighter-rouge"">cleanup-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It can be specified using the following notation: ""1 min"", ""20 s"""
CleanupOptions,cleanup-strategy.exponential-delay.attempts,"The number of times a failed cleanup is retried if <code class=""highlighter-rouge"">cleanup-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually."
RestOptions,rest.bind-address,The address that the server binds itself.
RestOptions,rest.bind-port,"The port that the server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine."
RestOptions,rest.url-prefix,"The url prefix that should be used by clients to construct the full target url, must start and end with '/'. This will be added between the address and version prefix. For example, if the option is set to '/foo/', the overview query URL will be transformed to 'localhost:8081/foo/v1/overview'. Attention: This option is respected only if the high-availability configuration is NONE."
RestOptions,rest.address,The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE.
RestOptions,rest.path,The path that should be used by clients to interact to the server which is accessible via URL.
RestOptions,rest.port,"The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE."
RestOptions,rest.await-leader-timeout,"The time that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint"
RestOptions,rest.retry.max-attempts,The number of retries the client will attempt if a retryable operations fails.
RestOptions,rest.retry.delay,The time that the client waits between retries (See also `rest.retry.max-attempts`).
RestOptions,rest.connection-timeout,The maximum time for the client to establish a TCP connection.
RestOptions,rest.idleness-timeout,The maximum time for a connection to stay idle before failing.
RestOptions,rest.server.max-content-length,The maximum content length in bytes that the server will handle.
RestOptions,rest.client.max-content-length,The maximum content length in bytes that the client will handle.
RestOptions,rest.server.numThreads,The number of threads for the asynchronous processing of requests.
RestOptions,rest.server.thread-priority,Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing.
RestOptions,rest.cache.checkpoint-statistics.timeout,"Duration from write after which cached checkpoints statistics are cleaned up. For backwards compatibility, if no value is configured, <code class=""highlighter-rouge"">web.refresh-interval</code> will be used instead."
RestOptions,rest.cache.checkpoint-statistics.size,Maximum number of entries in the checkpoint statistics cache.
RestOptions,rest.flamegraph.enabled,Enables the experimental flame graph feature.
RestOptions,rest.flamegraph.cleanup-interval,"Time after which cached stats are cleaned up if not accessed. It can be specified using notation: ""100 s"", ""10 m""."
RestOptions,rest.flamegraph.refresh-interval,"Time after which available stats are deprecated and need to be refreshed (by resampling).  It can be specified using notation: ""30 s"", ""1 m""."
RestOptions,rest.flamegraph.num-samples,Number of samples to take to build a FlameGraph.
RestOptions,rest.flamegraph.delay-between-samples,"Delay between individual stack trace samples taken for building a FlameGraph. It can be specified using notation: ""100 ms"", ""1 s""."
RestOptions,rest.flamegraph.stack-depth,Maximum depth of stack traces used to create FlameGraphs.
RestOptions,rest.async.store-duration,Maximum duration that the result of an async operation is stored. Once elapsed the result of the operation can no longer be retrieved.
RestOptions,rest.profiling.enabled,Enables the experimental profiler feature.
RestOptions,rest.profiling.history-size,Maximum profiling history instance to be maintained for JobManager or each TaskManager. The oldest instance will be removed on a rolling basis when the history size exceeds this value.
RestOptions,rest.profiling.duration-max,Maximum profiling duration for each profiling request. Any profiling request's duration exceeding this value will not be accepted.
RestOptions,rest.profiling.dir,Profiling result storing directory.
ExecutionOptions,execution.runtime-mode,"Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics."
ExecutionOptions,execution.batch-shuffle-mode,"Defines how data is exchanged between tasks in batch 'execution.runtime-mode' if the shuffling behavior has not been set explicitly for an individual exchange.<br />With pipelined exchanges, upstream and downstream tasks run simultaneously. In order to achieve lower latency, a result record is immediately sent to and processed by the downstream task. Thus, the receiver back-pressures the sender. The streaming mode always uses this exchange.<br />With blocking exchanges, upstream and downstream tasks run in stages. Records are persisted to some storage between stages. Downstream tasks then fetch these records after the upstream tasks finished. Such an exchange reduces the resources required to execute the job as it does not need to run upstream and downstream tasks simultaneously.<br />With hybrid exchanges (experimental), downstream tasks can run anytime as long as upstream tasks start running. When given sufficient resources, it can reduce the overall job execution time by running tasks simultaneously. Otherwise, it also allows jobs to be executed with very little resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies."
ExecutionOptions,execution.checkpointing.snapshot-compression,Tells if we should use compression for the state snapshot data or not
ExecutionOptions,execution.buffer-timeout.enabled,"If disabled, the config execution.buffer-timeout.interval will not take effect and the flushing will be triggered only when the output buffer is full thus maximizing throughput"
ExecutionOptions,execution.buffer-timeout.interval,"The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes:<ul><li>A positive value triggers flushing periodically by that interval</li><li>0 triggers flushing after every record thus minimizing latency</li><li>If the config execution.buffer-timeout.enabled is false, trigger flushing only when the output buffer is full thus maximizing throughput</li></ul>"
ExecutionOptions,execution.sort-partition.memory,"Sets the managed memory size for sort partition operator in NonKeyedPartitionWindowedStream.The memory size is only a weight hint. Thus, it will affect the operator's memory weight within a task, but the actual memory used depends on the running environment."
ExecutionOptions,execution.sort-keyed-partition.memory,"Sets the managed memory size for sort partition operator on KeyedPartitionWindowedStream.The memory size is only a weight hint. Thus, it will affect the operator's memory weight within a task, but the actual memory used depends on the running environment."
ExecutionOptions,execution.sorted-inputs.enabled,A flag to enable or disable sorting inputs of keyed operators. NOTE: It takes effect only in the BATCH runtime mode.
ExecutionOptions,execution.sorted-inputs.memory,"Sets the managed memory size for sorting inputs of keyed operators in BATCH runtime mode. The memory size is only a weight hint. Thus, it will affect the operator's memory weight within a task, but the actual memory used depends on the running environment."
ExecutionOptions,execution.batch-state-backend.enabled,A flag to enable or disable batch runtime specific state backend and timer service for keyed operators. NOTE: It takes effect only in the BATCH runtime mode and requires sorted inputsexecution.sorted-inputs.enabled to be enabled.
ExecutionOptions,execution.async-state.in-flight-records-limit,"The max limit of in-flight records number in async state execution, 'in-flight' refers to the records that have entered the operator but have not yet been processed and emitted to the downstream. If the in-flight records number exceeds the limit, the newly records entering will be blocked until the in-flight records number drops below the limit."
ExecutionOptions,execution.async-state.buffer-size,"The size of buffer under async state execution. Async state execution provides a buffer mechanism to reduce state access. When the number of state requests in the active buffer exceeds the batch size, a batched state execution would be triggered. Larger batch sizes will bring higher end-to-end latency, this option works with 'execution.async-state.buffer-timeout' to control the frequency of triggering."
ExecutionOptions,execution.async-state.buffer-timeout,"The timeout of buffer triggering in milliseconds. If the buffer has not reached the 'execution.async-state.buffer-size' within 'buffer-timeout' milliseconds, a trigger will perform actively."
HeartbeatManagerOptions,heartbeat.interval,Time interval between heartbeat RPC requests from the sender to the receiver side.
HeartbeatManagerOptions,heartbeat.timeout,Timeout for requesting and receiving heartbeats for both sender and receiver sides.
HeartbeatManagerOptions,heartbeat.rpc-failure-threshold,"The number of consecutive failed heartbeat RPCs until a heartbeat target is marked as unreachable. Failed heartbeat RPCs can be used to detect dead targets faster because they no longer receive the RPCs. The detection time is <code class=""highlighter-rouge"">heartbeat.interval</code> * <code class=""highlighter-rouge"">heartbeat.rpc-failure-threshold</code>. In environments with a flaky network, setting this value too low can produce false positives. In this case, we recommend to increase this value, but not higher than <code class=""highlighter-rouge"">heartbeat.timeout</code> / <code class=""highlighter-rouge"">heartbeat.interval</code>. The mechanism can be disabled by setting this option to <code class=""highlighter-rouge"">-1</code>"
JobManagerOptions,jobmanager.rpc.address,"The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers."
JobManagerOptions,jobmanager.bind-host,"The local address of the network interface that the job manager binds to. If not configured, '0.0.0.0' will be used."
JobManagerOptions,jobmanager.rpc.port,"The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers."
JobManagerOptions,jobmanager.rpc.bind-port,"The local RPC port that the JobManager binds to. If not configured, the external port (configured by 'jobmanager.rpc.port') will be used."
JobManagerOptions,jobmanager.heap.size,JVM heap size for the JobManager.
JobManagerOptions,jobmanager.heap.mb,JVM heap size (in megabytes) for the JobManager.
JobManagerOptions,jobmanager.memory.process.size,"Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration."
JobManagerOptions,jobmanager.memory.flink.size,"Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration."
JobManagerOptions,jobmanager.memory.heap.size,JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes).
JobManagerOptions,jobmanager.memory.off-heap.size,Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'.
JobManagerOptions,jobmanager.memory.enable-jvm-direct-memory-limit,Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option.
JobManagerOptions,jobmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the JobManager.
JobManagerOptions,jobmanager.memory.jvm-overhead.min,"Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."
JobManagerOptions,jobmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."
JobManagerOptions,jobmanager.memory.jvm-overhead.fraction,"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."
JobManagerOptions,jobmanager.execution.attempts-history-size,The maximum number of historical execution attempts kept in history.
JobManagerOptions,jobmanager.failure-enrichers,"An optional list of failure enricher names. If empty, NO failure enrichers will be started. If configured, only enrichers whose name matches any of the names in the list will be started."
JobManagerOptions,jobmanager.execution.failover-strategy,"This option specifies how the job computation recovers from task failures. Accepted values are:<ul><li>'full': Restarts all tasks to recover the job.</li><li>'region': Restarts all tasks that could be affected by the task failure. More details can be found <a href=""{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy"">here</a>.</li></ul>"
JobManagerOptions,jobmanager.archive.fs.dir,Directory for JobManager to store the archives of completed jobs.
JobManagerOptions,jobstore.cache-size,The job store cache size in bytes which is used to keep completed jobs in memory.
JobManagerOptions,jobstore.expiration-time,The time in seconds after which a completed job expires and is purged from the job store.
JobManagerOptions,jobstore.max-capacity,"The max number of completed jobs that can be kept in the job store. NOTICE: if memory store keeps too many jobs in session cluster, it may cause FullGC or OOM in jm."
JobManagerOptions,jobstore.type,"Determines which job store implementation is used in session cluster. Accepted values are:<ul><li>'File': the file job store keeps the archived execution graphs in files</li><li>'Memory': the memory job store keeps the archived execution graphs in memory. You may need to limit the <code class=""highlighter-rouge"">jobstore.max-capacity</code> to mitigate FullGC or OOM when there are too many graphs</li></ul>"
JobManagerOptions,jobmanager.retrieve-taskmanager-hostname,"Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to ""false"", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted."
JobManagerOptions,jobmanager.future-pool.size,"The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores."
JobManagerOptions,jobmanager.io-pool.size,"The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores."
JobManagerOptions,slot.request.timeout,The timeout for requesting a slot from Slot Pool.
JobManagerOptions,slot.idle.timeout,The timeout for a idle slot in Slot Pool.
JobManagerOptions,jobmanager.scheduler,"Determines which scheduler implementation is used to schedule tasks. If this option is not explicitly set, batch jobs will use the 'AdaptiveBatch' scheduler as the default, while streaming jobs will default to the 'Default' scheduler."
JobManagerOptions,scheduler-mode,"Determines the mode of the scheduler. Note that <code class=""highlighter-rouge"">scheduler-mode</code>=<code class=""highlighter-rouge"">REACTIVE</code> is only supported by standalone application deployments, not by active resource managers (YARN, Kubernetes) or session clusters."
JobManagerOptions,jobmanager.adaptive-scheduler.min-parallelism-increase,Configure the minimum increase in parallelism for a job to scale up.
JobManagerOptions,jobmanager.adaptive-scheduler.scaling-interval.min,Determines the minimum time between scaling operations.
JobManagerOptions,jobmanager.adaptive-scheduler.scaling-interval.max,"Determines the maximum interval time after which a scaling operation is forced even if the <code class=""highlighter-rouge"">jobmanager.adaptive-scheduler.min-parallelism-increase</code> aren't met. The scaling operation will be ignored when the resource hasn't changed. This option is disabled by default."
JobManagerOptions,jobmanager.adaptive-scheduler.resource-wait-timeout,"The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.<br />Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).<br />Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.<br />If <code class=""highlighter-rouge"">scheduler-mode</code> is configured to <code class=""highlighter-rouge"">REACTIVE</code>, this configuration value will default to a negative value to disable the resource timeout."
JobManagerOptions,jobmanager.adaptive-scheduler.resource-stabilization-timeout,"The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.<br />If <code class=""highlighter-rouge"">scheduler-mode</code> is configured to <code class=""highlighter-rouge"">REACTIVE</code>, this configuration value will default to 0, so that jobs are starting immediately with the available resources."
JobManagerOptions,jobmanager.partition.release-during-job-execution,Controls whether partitions should already be released during the job execution.
JobManagerOptions,jobmanager.adaptive-batch-scheduler.min-parallelism,"The lower bound of allowed parallelism to set adaptively if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>"
JobManagerOptions,jobmanager.adaptive-batch-scheduler.max-parallelism,"The upper bound of allowed parallelism to set adaptively if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>"
JobManagerOptions,jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task,"The average size of data volume to expect each task instance to process if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>. Note that when data skew occurs or the decided parallelism reaches the <code class=""highlighter-rouge"">jobmanager.adaptive-batch-scheduler.max-parallelism</code> (due to too much data), the data actually processed by some tasks may far exceed this value."
JobManagerOptions,jobmanager.adaptive-batch-scheduler.default-source-parallelism,"The default parallelism of source vertices if <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>"
JobManagerOptions,jobmanager.adaptive-batch-scheduler.speculative.enabled,Controls whether to enable speculative execution.
JobManagerOptions,jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions,"Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones."
JobManagerOptions,jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration,Controls how long an detected slow node should be blocked for.
JobManagerOptions,jobmanager.resource-id,"The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly."
JobManagerOptions,jobmanager.partition.hybrid.partition-data-consume-constraint,"Controls the constraint that hybrid partition data can be consumed. Note that this option is allowed only when <code class=""highlighter-rouge"">jobmanager.scheduler</code> has been set to <code class=""highlighter-rouge"">AdaptiveBatch</code>. Accepted values are:<ul><li>'<code class=""highlighter-rouge"">ALL_PRODUCERS_FINISHED</code>': hybrid partition data can be consumed only when all producers are finished.</li><li>'<code class=""highlighter-rouge"">ONLY_FINISHED_PRODUCERS</code>': hybrid partition data can be consumed when its producer is finished.</li><li>'<code class=""highlighter-rouge"">UNFINISHED_PRODUCERS</code>': hybrid partition data can be consumed even if its producer is un-finished.</li></ul>"
TraceOptions,traces.reporters,"An optional list of trace reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started."
TraceOptions,traces.report-events-as-spans,"Whether to report events as spans. This is a temporary parameter that is in place until we have support for reporting events. In the meantime, this can be activated to report them as spans instead."
TraceOptions,factory.class,The reporter factory class to use for the reporter named &lt;name&gt;.
TraceOptions,<parameter>,Configures the parameter &lt;parameter&gt; for the reporter named &lt;name&gt;.
TraceOptions,scope.variables.additional,The map of additional variables that should be included for the reporter named &lt;name&gt;.
HighAvailabilityOptions,high-availability.type,"Defines high-availability mode used for cluster execution. To enable high-availability, set this mode to ""ZOOKEEPER"", ""KUBERNETES"", or specify the fully qualified name of the factory class."
HighAvailabilityOptions,high-availability.cluster-id,"The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN."
HighAvailabilityOptions,high-availability.storageDir,File system path (URI) where Flink persists metadata in high-availability setups.
HighAvailabilityOptions,high-availability.jobmanager.port,"The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery."
HighAvailabilityOptions,high-availability.zookeeper.quorum,"The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper."
HighAvailabilityOptions,high-availability.zookeeper.path.root,The root path under which Flink stores its entries in ZooKeeper.
HighAvailabilityOptions,high-availability.zookeeper.path.jobgraphs,ZooKeeper root path (ZNode) for job graphs
HighAvailabilityOptions,high-availability.zookeeper.client.session-timeout,Defines the session timeout for the ZooKeeper session.
HighAvailabilityOptions,high-availability.zookeeper.client.connection-timeout,Defines the connection timeout for ZooKeeper.
HighAvailabilityOptions,high-availability.zookeeper.client.retry-wait,Defines the pause between consecutive retries.
HighAvailabilityOptions,high-availability.zookeeper.client.max-retry-attempts,Defines the number of connection retries before the client gives up.
HighAvailabilityOptions,high-availability.zookeeper.path.running-registry,Don't use this option anymore. It has no effect on Flink. The RunningJobRegistry has been replaced by the JobResultStore in Flink 1.15.
HighAvailabilityOptions,high-availability.zookeeper.client.acl,Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos).
HighAvailabilityOptions,high-availability.zookeeper.client.tolerate-suspended-connections,"Defines whether a suspended ZooKeeper connection will be treated as an error that causes the leader information to be invalidated or not. In case you set this option to <code class=""highlighter-rouge"">true</code>, Flink will wait until a ZooKeeper connection is marked as lost before it revokes the leadership of components. This has the effect that Flink is more resilient against temporary connection instabilities at the cost of running more likely into timing issues with ZooKeeper."
HighAvailabilityOptions,high-availability.zookeeper.client.ensemble-tracker,Defines whether Curator should enable ensemble tracker. This can be useful in certain scenarios in which CuratorFramework is accessing to ZK clusters via load balancer or Virtual IPs. Default Curator EnsembleTracking logic watches CuratorEventType.GET_CONFIG events and changes ZooKeeper connection string. It is not desired behaviour when ZooKeeper is running under the Virtual IPs. Under certain configurations EnsembleTracking can lead to setting of ZooKeeper connection string with unresolvable hostnames.
HighAvailabilityOptions,high-availability.zookeeper.client.authorization,"Add connection authorization Subsequent calls to this method overwrite the prior calls. In certain cases ZooKeeper requires additional Authorization information. For example list of valid names for ensemble in order to prevent accidentally connecting to a wrong ensemble. Each entry of type Map.Entry&lt;String, String&gt; will be transformed into an AuthInfo object with the constructor AuthInfo(String, byte[]). The field entry.key() will serve as the String scheme value, while the field entry.getValue() will be initially converted to a byte[] using the String#getBytes() method with UTF-8 encoding. If not set the default configuration for a Curator would be applied."
HighAvailabilityOptions,high-availability.zookeeper.client.max-close-wait,Defines the time Curator should wait during close to join background threads. If not set the default configuration for a Curator would be applied.
HighAvailabilityOptions,high-availability.zookeeper.client.simulated-session-expiration-percent,"The percentage set by this method determines how and if Curator will check for session expiration. See Curator documentation for <a href=""https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#simulatedSessionExpirationPercent(int)"">simulatedSessionExpirationPercent</a> property for more information."
HighAvailabilityOptions,high-availability.job.delay,The time before a JobManager after a fail over recovers the current jobs.
DeploymentOptionsInternal,$internal.deployment.config-dir,**DO NOT USE** The path to the configuration directory.
SecurityOptions,security.context.factory.classes,"List of factories that should be used to instantiate a security context. If multiple are configured, Flink will use the first compatible factory. You should have a NoOpSecurityContextFactory in this list as a fallback."
SecurityOptions,security.module.factory.classes,List of factories that should be used to instantiate security modules. All listed modules will be installed. Keep in mind that the configured security context might rely on some modules being present.
SecurityOptions,security.kerberos.login.principal,Kerberos principal name associated with the keytab.
SecurityOptions,security.kerberos.login.keytab,Absolute path to a Kerberos keytab file that contains the user credentials.
SecurityOptions,security.kerberos.krb5-conf.path,"Specify the local location of the krb5.conf file. If defined, this conf would be mounted on the JobManager and TaskManager containers/pods for Kubernetes and Yarn. Note: The KDC defined needs to be visible from inside the containers."
SecurityOptions,security.kerberos.login.use-ticket-cache,Indicates whether to read from your Kerberos ticket cache.
SecurityOptions,security.kerberos.login.contexts,"A comma-separated list of login contexts to provide the Kerberos credentials to (for example, `Client,KafkaClient` to use the credentials for ZooKeeper authentication and for Kafka authentication)"
SecurityOptions,security.kerberos.fetch.delegation-token,"Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens."
SecurityOptions,security.kerberos.relogin.period,The time period when keytab login happens automatically in order to always have a valid TGT.
SecurityOptions,security.kerberos.tokens.renewal.retry.backoff,The time period how long to wait before retrying to obtain new delegation tokens after a failure.
SecurityOptions,security.kerberos.tokens.renewal.time-ratio,Ratio of the tokens's expiration time when new credentials should be re-obtained.
SecurityOptions,security.kerberos.access.hadoopFileSystems,"A semicolon-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002;hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens."
SecurityOptions,security.delegation.tokens.enabled,Indicates whether to start delegation tokens system for external services.
SecurityOptions,security.delegation.tokens.renewal.retry.backoff,The time period how long to wait before retrying to obtain new delegation tokens after a failure.
SecurityOptions,security.delegation.tokens.renewal.time-ratio,Ratio of the tokens's expiration time when new credentials should be re-obtained.
SecurityOptions,enabled,"Controls whether to obtain credentials for services when security is enabled. By default, credentials for all supported services are retrieved when those services are configured, but it's possible to disable that behavior if it somehow conflicts with the application being run."
SecurityOptions,zookeeper.sasl.disable,
SecurityOptions,zookeeper.sasl.service-name,
SecurityOptions,zookeeper.sasl.login-context-name,
SecurityOptions,security.ssl.enabled,"Turns on SSL for internal and external network communication.This can be overridden by 'security.ssl.internal.enabled', 'security.ssl.external.enabled'. Specific internal components (rpc, data transport, blob server) may optionally override this through their own settings."
SecurityOptions,security.ssl.internal.enabled,"Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc)."
SecurityOptions,security.ssl.rest.enabled,Turns on SSL for external communication via the REST endpoints.
SecurityOptions,security.ssl.rest.authentication-enabled,Turns on mutual SSL authentication for external communication via the REST endpoints.
SecurityOptions,security.ssl.keystore,The Java keystore file to be used by the flink endpoint for its SSL Key and Certificate.
SecurityOptions,security.ssl.keystore-password,The secret to decrypt the keystore file.
SecurityOptions,security.ssl.key-password,The secret to decrypt the server key in the keystore.
SecurityOptions,security.ssl.truststore,The truststore file containing the public CA certificates to be used by flink endpoints to verify the peer’s certificate.
SecurityOptions,security.ssl.truststore-password,The secret to decrypt the truststore.
SecurityOptions,security.ssl.internal.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.keystore-password,"The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.key-password,"The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.keystore-type,"The type of keystore for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.truststore,"The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.truststore-password,"The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.truststore-type,"The type of truststore for Flink's internal endpoints (rpc, data transport, blob server)."
SecurityOptions,security.ssl.internal.cert.fingerprint,The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required
SecurityOptions,security.ssl.rest.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints."
SecurityOptions,security.ssl.rest.keystore-password,The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.key-password,The secret to decrypt the key in the keystore for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.keystore-type,The type of the keystore for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.truststore,The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.truststore-password,The password to decrypt the truststore for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.truststore-type,The type of the truststore for Flink's external REST endpoints.
SecurityOptions,security.ssl.rest.cert.fingerprint,The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA
SecurityOptions,security.ssl.protocol,The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list.
SecurityOptions,security.ssl.algorithms,"The comma separated list of standard SSL algorithms to be supported. Read more <a href=""http://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#ciphersuites"">here</a>"
SecurityOptions,security.ssl.verify-hostname,Flag to enable peer’s hostname verification during ssl handshake.
SecurityOptions,security.ssl.provider,"The SSL engine provider to use for the ssl transport:<ul><li><code class=""highlighter-rouge"">JDK</code>: default Java-based SSL engine</li><li><code class=""highlighter-rouge"">OPENSSL</code>: openSSL-based SSL engine using system libraries</li></ul><code class=""highlighter-rouge"">OPENSSL</code> is based on <a href=""http://netty.io/wiki/forked-tomcat-native.html#wiki-h2-4"">netty-tcnative</a> and comes in two flavours:<ul><li>dynamically linked: This will use your system's openSSL libraries (if compatible) and requires <code class=""highlighter-rouge"">opt/flink-shaded-netty-tcnative-dynamic-*.jar</code> to be copied to <code class=""highlighter-rouge"">lib/</code></li><li>statically linked: Due to potential licensing issues with openSSL (see <a href=""https://issues.apache.org/jira/browse/LEGAL-393"">LEGAL-393</a>), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into <code class=""highlighter-rouge"">lib/</code>:<br /><code class=""highlighter-rouge"">git clone https://github.com/apache/flink-shaded.git &amp;&amp; cd flink-shaded &amp;&amp; mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static</code></li></ul>"
SecurityOptions,security.ssl.internal.session-cache-size,"The size of the cache used for storing SSL session objects. According to <a href=""https://github.com/netty/netty/issues/832"">here</a>, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default)."
SecurityOptions,security.ssl.internal.session-timeout,The timeout (in ms) for the cached SSL session objects. (-1 = use system default)
SecurityOptions,security.ssl.internal.handshake-timeout,The timeout (in ms) during SSL handshake. (-1 = use system default)
SecurityOptions,security.ssl.internal.close-notify-flush-timeout,The timeout (in ms) for flushing the `close_notify` that was triggered by closing a channel. If the `close_notify` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default)
PipelineOptions,pipeline.name,The job name used for printing and logging.
PipelineOptions,pipeline.jars,A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths.
PipelineOptions,pipeline.classpaths,A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs.
PipelineOptions,pipeline.auto-generate-uids,"When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications.<br /><br />It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state."
PipelineOptions,pipeline.auto-type-registration,Controls whether Flink is automatically registering all types in the user programs with Kryo.
PipelineOptions,pipeline.auto-watermark-interval,"The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing."
PipelineOptions,pipeline.closure-cleaner-level,Configures the mode in which the closure cleaner works.
PipelineOptions,pipeline.force-avro,"Forces Flink to use the Apache Avro serializer for POJOs.<br /><br />Important: Make sure to include the <code class=""highlighter-rouge"">flink-avro</code> module."
PipelineOptions,pipeline.force-kryo,"If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO."
PipelineOptions,pipeline.force-kryo-avro,"Forces Flink to register avro classes in kryo serializer.<br /><br />Important: Make sure to include the flink-avro module. Otherwise, nothing will be registered. For backward compatibility, the default value is empty to conform to the behavior of the older version. That is, always register avro with kryo, and if flink-avro is not in the class path, register a dummy serializer. In Flink-2.0, we will set the default value to true."
PipelineOptions,pipeline.generic-types,"If the use of generic types is disabled, Flink will throw an <code class=""highlighter-rouge"">UnsupportedOperationException</code> whenever it encounters a data type that would go through Kryo for serialization.<br /><br />Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used.<br /><br />We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail."
PipelineOptions,pipeline.global-job-parameters,"Register a custom, serializable user configuration object. The configuration can be  accessed in operators"
PipelineOptions,pipeline.jobvertex-parallelism-overrides,A parallelism override map (jobVertexId -&gt; parallelism) which will be used to update the parallelism of the corresponding job vertices of submitted JobGraphs.
PipelineOptions,pipeline.max-parallelism,The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state. Changing the value explicitly when recovery from original job will lead to state incompatibility. Must be less than or equal to 32768.
PipelineOptions,pipeline.object-reuse,When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour.
PipelineOptions,pipeline.default-kryo-serializers,"Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializers<br /><br />Example:<br /><code class=""highlighter-rouge"">class:org.example.ExampleClass,serializer:org.example.ExampleSerializer1; class:org.example.ExampleClass2,serializer:org.example.ExampleSerializer2</code>"
PipelineOptions,pipeline.registered-kryo-types,"Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written."
PipelineOptions,pipeline.registered-pojo-types,"Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written."
PipelineOptions,pipeline.serialization-config,"List of pairs of class names and serializer configs to be used. There is a <code class=""highlighter-rouge"">type</code> field in the serializer config and each type has its own configuration. Note: only standard YAML config parser is supported, please use ""config.yaml"" as the config file. The fields involved are:<ul><li><code class=""highlighter-rouge"">type</code>: the serializer type which could be ""pojo"", ""kryo"" or ""typeinfo"". If the serializer type is ""pojo"" or ""kryo"" without field <code class=""highlighter-rouge"">kryo-type</code>, it means the data type will use POJO or Kryo serializer directly.</li><li><code class=""highlighter-rouge"">kryo-type</code>: the Kryo serializer type which could be ""default"" or ""registered"". The Kryo serializer will use the serializer for the data type  as default serializers when the kryo-type is ""default"", and register the data type and its serializer to Kryo serializer when the kryo-type is registered. When the field exists, there must be a field <code class=""highlighter-rouge"">class</code> to specify the serializer class name.</li><li><code class=""highlighter-rouge"">class</code>: the serializer class name for type ""kryo"" or ""typeinfo"". For ""kryo"", it should be a subclass of <code class=""highlighter-rouge"">com.esotericsoftware.kryo.Serializer</code>. For ""typeinfo"", it should be a subclass of <code class=""highlighter-rouge"">org.apache.flink.api.common.typeinfo.TypeInfoFactory</code>.</li></ul>Example:<br /><code class=""highlighter-rouge"">[org.example.ExampleClass1: {type: pojo}, org.example.ExampleClass2: {type: kryo}, org.example.ExampleClass3: {type: kryo, kryo-type: default, class: org.example.Class3KryoSerializer}, org.example.ExampleClass4: {type: kryo, kryo-type: registered, class: org.example.Class4KryoSerializer}, org.example.ExampleClass5: {type: typeinfo, class: org.example.Class5TypeInfoFactory}]</code>"
PipelineOptions,pipeline.operator-chaining.enabled,Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization.
PipelineOptions,pipeline.operator-chaining.chain-operators-with-different-max-parallelism,Operators with different max parallelism can be chained together. Default behavior may prevent rescaling when the AdaptiveScheduler is used.
PipelineOptions,pipeline.cached-files,"Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.<br /><br />Example:<br /><code class=""highlighter-rouge"">name:file1,path:'file:///tmp/file1';name:file2,path:'hdfs:///tmp/file2'</code>"
PipelineOptions,pipeline.vertex-description-mode,The mode how we organize description of a job vertex.
PipelineOptions,pipeline.vertex-name-include-index-prefix,"Whether name of vertex includes topological index or not. When it is true, the name will have a prefix of index of the vertex, like '[vertex-0]Source: source'. It is false by default"
PipelineOptions,pipeline.watermark-alignment.allow-unaligned-source-splits,"If watermark alignment is used, sources with multiple splits will attempt to pause/resume split readers to avoid watermark drift of source splits. However, if split readers don't support pause/resume, an UnsupportedOperationException will be thrown when there is an attempt to pause/resume. To allow use of split readers that don't support pause/resume and, hence, to allow unaligned splits while still using watermark alignment, set this parameter to true. The default value is false. Note: This parameter may be removed in future releases."
BlobServerOptions,blob.storage.directory,"The config parameter defining the local storage directory to be used by the blob server. If not configured, then it will default to &lt;WORKING_DIR&gt;/blobStorage."
BlobServerOptions,blob.fetch.retries,The config parameter defining number of retires for failed BLOB fetches.
BlobServerOptions,blob.fetch.num-concurrent,The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves.
BlobServerOptions,blob.fetch.backlog,"The config parameter defining the desired backlog of BLOB fetches on the JobManager.Note that the operating system usually enforces an upper limit on the backlog size based on the <code class=""highlighter-rouge"">SOMAXCONN</code> setting."
BlobServerOptions,blob.server.port,The config parameter defining the server port of the blob service.
BlobServerOptions,blob.service.ssl.enabled,Flag to override ssl support for the blob service transport.
BlobServerOptions,blob.service.cleanup.interval,Cleanup interval of the blob caches at the task managers (in seconds).
BlobServerOptions,blob.offload.minsize,The minimum size for messages to be offloaded to the BlobServer.
BlobServerOptions,blob.client.socket.timeout,The socket timeout in milliseconds for the blob client.
BlobServerOptions,blob.client.connect.timeout,The connection timeout in milliseconds for the blob client.
MetricOptions,metrics.reporters,"An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started."
MetricOptions,class,The reporter class to use for the reporter named &lt;name&gt;.
MetricOptions,factory.class,The reporter factory class to use for the reporter named &lt;name&gt;.
MetricOptions,interval,The reporter interval to use for the reporter named &lt;name&gt;. Only applicable to push-based reporters.
MetricOptions,scope.delimiter,The delimiter used to assemble the metric identifier for the reporter named &lt;name&gt;.
MetricOptions,scope.variables.additional,The map of additional variables that should be included for the reporter named &lt;name&gt;. Only applicable to tag-based reporters.
MetricOptions,scope.variables.excludes,The set of variables that should be excluded for the reporter named &lt;name&gt;. Only applicable to tag-based reporters.
MetricOptions,filter.includes,"The metrics that should be included for the reporter named &lt;name&gt;. Filters are specified as a list, with each filter following this format:<br /><code class=""highlighter-rouge"">&lt;scope&gt;[:&lt;name&gt;[,&lt;name&gt;][:&lt;type&gt;[,&lt;type&gt;]]]</code><br />A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.<br /><ul><li>scope: Filters based on the logical scope.<br />Specified as a pattern where <code class=""highlighter-rouge"">*</code> matches any sequence of characters and <code class=""highlighter-rouge"">.</code> separates scope components.<br /><br />For example:<br /> ""<code class=""highlighter-rouge"">jobmanager.job</code>"" matches any job-related metrics on the JobManager,<br /> ""<code class=""highlighter-rouge"">*.job</code>"" matches all job-related metrics and<br /> ""<code class=""highlighter-rouge"">*.job.*</code>"" matches all metrics below the job-level (i.e., task/operator metrics etc.).<br /><br /></li><li>name: Filters based on the metric name.<br />Specified as a comma-separate list of patterns where <code class=""highlighter-rouge"">*</code> matches any sequence of characters.<br /><br />For example, ""<code class=""highlighter-rouge"">*Records*,*Bytes*</code>"" matches any metrics where the name contains <code class=""highlighter-rouge"">""Records"" or ""Bytes""</code>.<br /><br /></li><li>type: Filters based on the metric type. Specified as a comma-separated list of metric types: <code class=""highlighter-rouge"">[counter, meter, gauge, histogram]</code></li></ul>Examples:<ul><li>""<code class=""highlighter-rouge"">*:numRecords*</code>"" Matches metrics like <code class=""highlighter-rouge"">numRecordsIn</code>.</li><li>""<code class=""highlighter-rouge"">*.job.task.operator:numRecords*</code>"" Matches metrics like <code class=""highlighter-rouge"">numRecordsIn</code> on the operator level.</li><li>""<code class=""highlighter-rouge"">*.job.task.operator:numRecords*:meter</code>"" Matches meter metrics like <code class=""highlighter-rouge"">numRecordsInPerSecond</code> on the operator level.</li><li>""<code class=""highlighter-rouge"">*:numRecords*,numBytes*:counter,meter</code>"" Matches all counter/meter metrics like or <code class=""highlighter-rouge"">numRecordsInPerSecond</code>.</li></ul>"
MetricOptions,filter.excludes,"The metrics that should be excluded for the reporter named &lt;name&gt;. The format is identical to <code class=""highlighter-rouge"">filter.includes</code><br />"
MetricOptions,<parameter>,Configures the parameter &lt;parameter&gt; for the reporter named &lt;name&gt;.
MetricOptions,metrics.scope.delimiter,Delimiter used to assemble the metric identifier.
MetricOptions,metrics.scope.jm,Defines the scope format string that is applied to all metrics scoped to a JobManager. Only effective when a identifier-based reporter is configured.
MetricOptions,metrics.scope.tm,Defines the scope format string that is applied to all metrics scoped to a TaskManager. Only effective when a identifier-based reporter is configured
MetricOptions,metrics.scope.jm-job,Defines the scope format string that is applied to all metrics scoped to a job on a JobManager. Only effective when a identifier-based reporter is configured
MetricOptions,metrics.scope.jm-operator,"Defines the scope format string that is applied to all metrics scoped to the components running on a JobManager of an Operator, like OperatorCoordinator for Source Enumerator metrics."
MetricOptions,metrics.scope.tm-job,Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager. Only effective when a identifier-based reporter is configured
MetricOptions,metrics.scope.task,Defines the scope format string that is applied to all metrics scoped to a task. Only effective when a identifier-based reporter is configured
MetricOptions,metrics.scope.operator,Defines the scope format string that is applied to all metrics scoped to an operator. Only effective when a identifier-based reporter is configured
MetricOptions,metrics.latency.interval,Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster.
MetricOptions,metrics.latency.granularity,"Defines the granularity of latency metrics. Accepted values are:<ul><li>single - Track latency without differentiating between sources and subtasks.</li><li>operator - Track latency while differentiating between sources, but not subtasks.</li><li>subtask - Track latency while differentiating between sources and subtasks.</li></ul>"
MetricOptions,metrics.latency.history-size,Defines the number of measured latencies to maintain at each operator.
MetricOptions,metrics.system-resource,"Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage."
MetricOptions,metrics.system-resource-probing-interval,Interval between probing of system resource metrics specified. Has an effect only when 'metrics.system-resource' is enabled.
MetricOptions,metrics.internal.query-service.port,"The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port."
MetricOptions,metrics.internal.query-service.thread-priority,"The thread priority used for Flink's internal metric query service. The thread is created by Pekko's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down."
MetricOptions,metrics.fetcher.update-interval,Update interval for the metric fetcher used by the web UI. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely.
MetricOptions,metrics.job.status.enable,The selection of job status metrics that should be reported.
ExternalResourceOptions,external-resources,"List of the &lt;resource_name&gt; of all external resources with delimiter "";"", e.g. ""gpu;fpga"" for two external resource gpu and fpga. The &lt;resource_name&gt; will be used to splice related config options for external resource. Only the &lt;resource_name&gt; defined here will go into effect by external resource framework. Do not set the &lt;resource_name&gt; to 'none', which is preserved internally"
ExternalResourceOptions,external-resource.<resource_name>.driver-factory.class,"Defines the factory class name for the external resource identified by &lt;resource_name&gt;. The factory will be used to instantiated the ExternalResourceDriver at the TaskExecutor side. For example, org.apache.flink.externalresource.gpu.GPUDriverFactory"
ExternalResourceOptions,external-resource.<resource_name>.amount,The amount for the external resource specified by &lt;resource_name&gt; per TaskExecutor.
ExternalResourceOptions,external-resource.<resource_name>.param.<param>,The naming pattern of custom config options for the external resource specified by &lt;resource_name&gt;. Only the configurations that follow this pattern would be passed into the driver factory of that external resource.
AkkaOptions,pekko.ask.callstack,"If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint."
AkkaOptions,pekko.ask.timeout,Timeout used for all futures and blocking Pekko calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d).
AkkaOptions,pekko.ask.timeout,Timeout used for all futures and blocking Pekko calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d).
AkkaOptions,pekko.tcp.timeout,"Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value."
AkkaOptions,pekko.startup-timeout,Timeout after which the startup of a remote component is considered being failed.
AkkaOptions,pekko.ssl.enabled,Turns on SSL for Pekko’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true.
AkkaOptions,pekko.framesize,"Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier."
AkkaOptions,pekko.throughput,Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness.
AkkaOptions,pekko.log.lifecycle.events,Turns on the Pekko’s remote logging of events. Set this value to 'true' in case of debugging.
AkkaOptions,pekko.lookup.timeout,Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).
AkkaOptions,pekko.lookup.timeout,Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).
AkkaOptions,akka.client.timeout,"DEPRECATED: Use the ""client.timeout"" instead. Timeout for all blocking calls on the client side."
AkkaOptions,pekko.jvm-exit-on-fatal-error,Exit JVM on fatal Pekko errors.
AkkaOptions,pekko.retry-gate-closed-for,Milliseconds a gate should be closed for after a remote connection was disconnected.
AkkaOptions,pekko.fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.
AkkaOptions,pekko.fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.
AkkaOptions,pekko.fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.
AkkaOptions,pekko.remote-fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.
AkkaOptions,pekko.remote-fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.
AkkaOptions,pekko.remote-fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.
AkkaOptions,pekko.client-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.
AkkaOptions,pekko.client-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.
AkkaOptions,pekko.client-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
AkkaOptions,pekko.server-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.
AkkaOptions,pekko.server-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.
AkkaOptions,pekko.server-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
AkkaOptions,akka.watch.heartbeat.interval,"Heartbeat interval for Akka’s DeathWatch mechanism to detect dead TaskManagers. If TaskManagers are wrongly marked dead because of lost or delayed heartbeat messages, then you should decrease this value or increase akka.watch.heartbeat.pause. A thorough description of Akka’s DeathWatch can be found <a href=""https://pekko.apache.org/docs/pekko/current/remoting-artery.html#failure-detector"">here</a>"
AkkaOptions,akka.watch.heartbeat.pause,"Acceptable heartbeat pause for Akka’s DeathWatch mechanism. A low value does not allow an irregular heartbeat. If TaskManagers are wrongly marked dead because of lost or delayed heartbeat messages, then you should increase this value or decrease akka.watch.heartbeat.interval. Higher value increases the time to detect a dead TaskManager. A thorough description of Akka’s DeathWatch can be found <a href=""https://pekko.apache.org/docs/pekko/current/remoting-artery.html#failure-detector"">here</a>"
AkkaOptions,akka.watch.threshold,"Threshold for the DeathWatch failure detector. A low value is prone to false positives whereas a high value increases the time to detect a dead TaskManager. A thorough description of Akka’s DeathWatch can be found <a href=""https://pekko.apache.org/docs/pekko/current/remoting-artery.html#failure-detector"">here</a>"
RestartStrategyOptions,restart-strategy.type,"Defines the restart strategy to use in case of job failures.<br />Accepted values are:<ul><li><code class=""highlighter-rouge"">disable</code>, <code class=""highlighter-rouge"">off</code>, <code class=""highlighter-rouge"">none</code>: No restart strategy.</li><li><code class=""highlighter-rouge"">fixed-delay</code>, <code class=""highlighter-rouge"">fixeddelay</code>: Fixed delay restart strategy. More details can be found <a href=""{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy"">here</a>.</li><li><code class=""highlighter-rouge"">failure-rate</code>, <code class=""highlighter-rouge"">failurerate</code>: Failure rate restart strategy. More details can be found <a href=""{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy"">here</a>.</li><li><code class=""highlighter-rouge"">exponential-delay</code>, <code class=""highlighter-rouge"">exponentialdelay</code>: Exponential delay restart strategy. More details can be found <a href=""{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy"">here</a>.</li></ul>If checkpointing is disabled, the default value is <code class=""highlighter-rouge"">disable</code>. If checkpointing is enabled, the default value is <code class=""highlighter-rouge"">exponential-delay</code>, and the default values of <code class=""highlighter-rouge"">exponential-delay</code> related config options will be used."
RestartStrategyOptions,restart-strategy.fixed-delay.attempts,"The number of times that Flink retries the execution before the job is declared as failed if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">fixed-delay</code>."
RestartStrategyOptions,restart-strategy.fixed-delay.delay,"Delay between two consecutive restart attempts if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">fixed-delay</code>. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.failure-rate.max-failures-per-interval,"Maximum number of restarts in given time interval before failing a job if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">failure-rate</code>."
RestartStrategyOptions,restart-strategy.failure-rate.failure-rate-interval,"Time interval for measuring failure rate if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">failure-rate</code>. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.failure-rate.delay,"Delay between two consecutive restart attempts if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">failure-rate</code>. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.exponential-delay.initial-backoff,"Starting duration between restarts if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.exponential-delay.max-backoff,"The highest possible duration between restarts if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.exponential-delay.backoff-multiplier,"Backoff value is multiplied by this value after every failure,until max backoff is reached if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>."
RestartStrategyOptions,restart-strategy.exponential-delay.reset-backoff-threshold,"Threshold when the backoff is reset to its initial value if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It specifies how long the job must be running without failure to reset the exponentially increasing backoff to its initial value. It can be specified using notation: ""1 min"", ""20 s"""
RestartStrategyOptions,restart-strategy.exponential-delay.jitter-factor,"Jitter specified as a portion of the backoff if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. It represents how large random value will be added or subtracted to the backoff. Useful when you want to avoid restarting multiple jobs at the same time."
RestartStrategyOptions,restart-strategy.exponential-delay.attempts-before-reset-backoff,"The number of times that Flink retries the execution before failing the job if <code class=""highlighter-rouge"">restart-strategy.type</code> has been set to <code class=""highlighter-rouge"">exponential-delay</code>. The number will be reset once the backoff is reset to its initial value."
JMXServerOptions,jmx.server.port,"The port range for the JMX server to start the registry. The port config can be a single port: ""9123"", a range of ports: ""50100-50200"", or a list of ranges and ports: ""50100-50200,50300-50400,51234"". <br />This option overrides metrics.reporter.*.port option."
StateLatencyTrackOptions,state.latency-track.keyed-state-enabled,"Whether to track latency of keyed state operations, e.g value state put/get/clear."
StateLatencyTrackOptions,state.latency-track.sample-interval,"The sample interval of latency track once 'state.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests."
StateLatencyTrackOptions,state.latency-track.history-size,Defines the number of measured latencies to maintain at each state access operation.
StateLatencyTrackOptions,state.latency-track.state-name-as-variable,Whether to expose state name as a variable if tracking latency.
SlowTaskDetectorOptions,slow-task-detector.check-interval,The interval to check slow tasks.
SlowTaskDetectorOptions,slow-task-detector.execution-time.baseline-lower-bound,The lower bound of slow task detection baseline.
SlowTaskDetectorOptions,slow-task-detector.execution-time.baseline-ratio,"The finished execution ratio threshold to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. Note that the execution time will be weighted with the task's input bytes to ensure the accuracy of the detection if data skew occurs."
SlowTaskDetectorOptions,slow-task-detector.execution-time.baseline-multiplier,"The multiplier to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. Note that the execution time will be weighted with the task's input bytes to ensure the accuracy of the detection if data skew occurs."
NettyShuffleEnvironmentOptions,taskmanager.data.port,The task manager’s external port used for data exchange operations.
NettyShuffleEnvironmentOptions,taskmanager.data.bind-port,"The task manager's bind port used for data exchange operations. Also accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. If not configured, 'taskmanager.data.port' will be used."
NettyShuffleEnvironmentOptions,taskmanager.data.ssl.enabled,Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true
NettyShuffleEnvironmentOptions,taskmanager.network.batch-shuffle.compression.enabled,"Boolean flag indicating whether the shuffle data will be compressed for batch shuffle mode. Note that data is compressed per buffer and compression can incur extra CPU overhead, so it is more effective for IO bounded scenario when compression ratio is high."
NettyShuffleEnvironmentOptions,taskmanager.network.compression.codec,"The codec to be used when compressing shuffle data. If it is ""NONE"", compression is disable. If it is not ""NONE"", only ""LZ4"", ""LZO"" and ""ZSTD"" are supported now. Through tpc-ds test of these three algorithms, the results show that ""LZ4"" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. ""ZSTD"" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future."
NettyShuffleEnvironmentOptions,taskmanager.network.detailed-metrics,Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths.
NettyShuffleEnvironmentOptions,taskmanager.network.numberOfBuffers,
NettyShuffleEnvironmentOptions,taskmanager.network.memory.fraction,"Fraction of JVM memory to use for network buffers. This determines how many streaming data exchange channels a TaskManager can have at the same time and how well buffered the channels are. If a job is rejected or you get a warning that the system has not enough buffers available, increase this value or the min/max values below. Also note, that ""taskmanager.network.memory.min""` and ""taskmanager.network.memory.max"" may override this fraction."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.min,Minimum memory size for network buffers.
NettyShuffleEnvironmentOptions,taskmanager.network.memory.max,Maximum memory size for network buffers.
NettyShuffleEnvironmentOptions,taskmanager.network.max-num-tcp-connections,The maximum number of tpc connections between taskmanagers for data communication.
NettyShuffleEnvironmentOptions,taskmanager.network.memory.read-buffer.required-per-gate.max,"The maximum number of network read buffers that are required by an input gate. (An input gate is responsible for reading data from all subtasks of an upstream task.) The number of buffers needed by an input gate is dynamically calculated in runtime, depending on various factors (e.g., the parallelism of the upstream task). Among the calculated number of needed buffers, the part below this configured value is required, while the excess part, if any, is optional. A task will fail if the required buffers cannot be obtained in runtime. A task will not fail due to not obtaining optional buffers, but may suffer a performance reduction. If not explicitly configured, the default value is Integer.MAX_VALUE for streaming workloads, and 1000 for batch workloads. If explicitly configured, the configured value should be at least 1."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.buffers-per-channel,"Number of exclusive network buffers for each outgoing/incoming channel (subpartition/input channel) in the credit-based flow control model. For the outgoing channel(subpartition), this value is the effective exclusive buffers per channel. For the incoming channel(input channel), this value is the max number of exclusive buffers per channel, the number of effective exclusive network buffers per channel is dynamically calculated from taskmanager.network.memory.read-buffer.required-per-gate.max and the effective range is from 0 to the configured value. The minimum valid value for the option is 0. When the option is configured as 0, the exclusive network buffers used by downstream incoming channel will be 0, but for each upstream outgoing channel, max(1, configured value) will be used. In other words, we ensure that, for performance reasons, at least one buffer is used per outgoing channel regardless of the configuration."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.floating-buffers-per-gate,"Number of floating network buffers for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the channels. The floating buffers can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. For the outgoing gate(result partition), this value is the effective floating buffers per gate. For the incoming gate(input gate), this value is a recommended number of floating buffers, the number of effective floating network buffers per gate is dynamically calculated from taskmanager.network.memory.read-buffer.required-per-gate.max and the range of effective floating buffers is from 0 to (parallelism - 1)."
NettyShuffleEnvironmentOptions,taskmanager.network.sort-shuffle.min-buffers,"Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value."
NettyShuffleEnvironmentOptions,taskmanager.network.sort-shuffle.min-parallelism,"Parallelism threshold to switch between sort-based blocking shuffle and hash-based blocking shuffle, which means for batch jobs of smaller parallelism, hash-shuffle will be used and for batch jobs of larger or equal parallelism, sort-shuffle will be used. The value 1 means that sort-shuffle is the default option. Note: For production usage, you may also need to tune 'taskmanager.network.sort-shuffle.min-buffers' and 'taskmanager.memory.framework.off-heap.batch-shuffle.size' for better performance."
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.spill-index-region-group-size,Controls the region group size(in bytes) of hybrid spilled file data index. Note: This option will be ignored if taskmanager.network.hybrid-shuffle.enable-new-mode is set true.
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.num-retained-in-memory-regions-max,Controls the max number of hybrid retained regions in memory. Note: This option will be ignored if taskmanager.network.hybrid-shuffle.enable-new-mode is set true.
NettyShuffleEnvironmentOptions,taskmanager.network.memory.max-buffers-per-channel,"Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.max-overdraft-buffers-per-gate,"Number of max overdraft network buffers to use for each ResultPartition. The overdraft buffers will be used when the subtask cannot apply to the normal buffers  due to back pressure, while subtask is performing an action that can not be interrupted in the middle,  like serializing a large record, flatMap operator producing multiple records for one single input record or processing time timer producing large output. In situations like that system will allow subtask to request overdraft buffers, so that the subtask can finish such uninterruptible action, without blocking unaligned checkpoints for long period of time. Overdraft buffers are provided on best effort basis only if the system has some unused buffers available. Subtask that has used overdraft buffers won't be allowed to process any more records until the overdraft buffers are returned to the pool. It should be noted that this config option only takes effect for Pipelined Shuffle."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.exclusive-buffers-request-timeout-ms,"The timeout for requesting exclusive buffers for each channel. Since the number of maximum buffers and the number of required buffers is not the same for local buffer pools, there may be deadlock cases that the upstreamtasks have occupied all the buffers and the downstream tasks are waiting for the exclusive buffers. The timeout breaksthe tie by failing the request of exclusive buffers and ask users to increase the number of total buffers."
NettyShuffleEnvironmentOptions,taskmanager.network.memory.buffers-request-timeout,"The timeout for requesting buffers for each channel. Since the number of maximum buffers and the number of required buffers is not the same for local buffer pools, there may be deadlock cases that the upstreamtasks have occupied all the buffers and the downstream tasks are waiting for the exclusive buffers. The timeout breaksthe tie by failing the request of exclusive buffers and ask users to increase the number of total buffers."
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.enable-new-mode,"The option is used to enable the new mode of hybrid shuffle, which has resolved existing issues in the legacy mode. First, the new mode uses less required network memory. Second, the new mode can store shuffle data in remote storage when the disk space is not enough, which could avoid insufficient disk space errors and is only supported when taskmanager.network.hybrid-shuffle.remote.path is configured. The new mode is currently in an experimental phase. It can be set to false to fallback to the legacy mode  if something unexpected. Once the new mode reaches a stable state, the legacy mode as well as the option will be removed."
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.external-remote-tier-factory.class,"The option configures the class that is responsible for creating an external remote tier factory for hybrid shuffle. If configured, the hybrid shuffle will only initialize the specified remote tier according to the given class name. Currently, since the tier interfaces are not yet public and are still actively evolving, it is recommended that users do not independently implement the external remote tier until the tier interfaces are stabilized."
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.remote.path,"The option is used to configure the base path of remote storage for hybrid shuffle. The shuffle data will be stored in remote storage when the disk space is not enough. Note: If the option is configured and taskmanager.network.hybrid-shuffle.enable-new-mode is false, this option will be ignored. If the option is not configured and taskmanager.network.hybrid-shuffle.enable-new-mode is true, the remote storage will be disabled."
NettyShuffleEnvironmentOptions,taskmanager.network.hybrid-shuffle.memory-decoupling.enabled,"The option is used to make the memory that is used by hybrid shuffle decoupled from the complexity of the job topology and the number of tasks on the task manager. It significantly reduces the chance of the ""Insufficient number of network buffers"" exception, while the workloads may suffer performance reduction silently."
NettyShuffleEnvironmentOptions,taskmanager.network.blocking-shuffle.type,"The blocking shuffle type, either ""mmap"" or ""file"". The ""auto"" means selecting the property type automatically based on system memory architecture (64 bit for mmap and 32 bit for file). Note that the memory usage of mmap is not accounted by configured memory limits, but some resource frameworks like yarn would track this memory usage and kill the container once memory exceeding some threshold. Also note that this option is experimental and might be changed future."
NettyShuffleEnvironmentOptions,taskmanager.network.tcp-connection.enable-reuse-across-jobs,"Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. Note that to avoid connection leak, you must set taskmanager.network.max-num-tcp-connections to a smaller value before you enable tcp connection reuse."
NettyShuffleEnvironmentOptions,taskmanager.network.netty.num-arenas,The number of Netty arenas.
NettyShuffleEnvironmentOptions,taskmanager.network.netty.server.numThreads,The number of Netty server threads.
NettyShuffleEnvironmentOptions,taskmanager.network.netty.client.numThreads,The number of Netty client threads.
NettyShuffleEnvironmentOptions,taskmanager.network.netty.server.backlog,The netty server connection backlog.
NettyShuffleEnvironmentOptions,taskmanager.network.netty.client.connectTimeoutSec,The Netty client connection timeout.
NettyShuffleEnvironmentOptions,taskmanager.network.retries,The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections
NettyShuffleEnvironmentOptions,taskmanager.network.netty.sendReceiveBufferSize,The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux.
NettyShuffleEnvironmentOptions,taskmanager.network.netty.transport,"The Netty transport type, either ""nio"" or ""epoll"". The ""auto"" means selecting the property mode automatically based on the platform. Note that the ""epoll"" mode can get better performance, less GC and have more advanced features which are only available on modern Linux."
NettyShuffleEnvironmentOptions,taskmanager.network.netty.client.tcp.keepIdleSec,"The time (in seconds) the connection needs to remain idle before TCP starts sending keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298."
NettyShuffleEnvironmentOptions,taskmanager.network.netty.client.tcp.keepIntervalSec,"The time (in seconds) between individual keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298."
NettyShuffleEnvironmentOptions,taskmanager.network.netty.client.tcp.keepCount,"The maximum number of keepalive probes TCP should send before Netty client dropping the connection. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298."
NettyShuffleEnvironmentOptions,taskmanager.network.request-backoff.initial,Minimum backoff in milliseconds for partition requests of local input channels.
NettyShuffleEnvironmentOptions,taskmanager.network.request-backoff.max,Maximum backoff in milliseconds for partition requests of local input channels.
NettyShuffleEnvironmentOptions,taskmanager.network.partition-request-timeout,"Timeout for an individual partition request of remote input channels. The partition request will finally fail if the total wait time exceeds twice the value of <code class=""highlighter-rouge"">taskmanager.network.request-backoff.max</code>."
OptimizerOptions,compiler.delimited-informat.max-line-samples,The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters.
OptimizerOptions,compiler.delimited-informat.min-line-samples,The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters
OptimizerOptions,compiler.delimited-informat.max-sample-len,"The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format’s parameters."
StateChangelogOptions,state.changelog.periodic-materialize.enabled,"Defines whether to enable periodic materialization, all changelogs will not be truncated which may increase the space of checkpoint if disabled"
StateChangelogOptions,state.changelog.periodic-materialize.interval,Defines the interval in milliseconds to perform periodic materialization for state backend. It only takes effect when state.changelog.periodic-materialize.enabled is true
StateChangelogOptions,state.changelog.max-failures-allowed,Max number of consecutive materialization failures allowed.
StateChangelogOptions,state.changelog.enabled,"Whether to enable state backend to write state changes to StateChangelog. If this config is not set explicitly, it means no preference for enabling the change log, and the value in lower config level will take effect. The default value 'false' here means if no value set (job or cluster), the change log will not be enabled."
StateChangelogOptions,state.changelog.storage,The storage to be used to store state changelog.<br />The implementation can be specified via their shortcut name.<br />The list of recognized shortcut names currently includes 'memory' and 'filesystem'.
JobEventStoreOptions,job-event.store.write-buffer.size,The size of the write buffer of JobEventStore. The content will be flushed to external file system once the buffer is full
JobEventStoreOptions,job-event.store.write-buffer.flush-interval,The flush interval of JobEventStore write buffers. Buffer contents will be flushed to external file system regularly with regard to this value.
QueryableStateOptions,queryable-state.proxy.ports,"The port range of the queryable state proxy. The specified range can be a single port: ""9123"", a range of ports: ""50100-50200"", or a list of ranges and ports: ""50100-50200,50300-50400,51234""."
QueryableStateOptions,queryable-state.proxy.network-threads,Number of network (Netty's event loop) Threads for queryable state proxy.
QueryableStateOptions,queryable-state.proxy.query-threads,Number of query Threads for queryable state proxy. Uses the number of slots if set to 0.
QueryableStateOptions,queryable-state.server.ports,"The port range of the queryable state server. The specified range can be a single port: ""9123"", a range of ports: ""50100-50200"", or a list of ranges and ports: ""50100-50200,50300-50400,51234""."
QueryableStateOptions,queryable-state.server.network-threads,Number of network (Netty's event loop) Threads for queryable state server.
QueryableStateOptions,queryable-state.server.query-threads,Number of query Threads for queryable state server. Uses the number of slots if set to 0.
QueryableStateOptions,queryable-state.enable,Option whether the queryable state proxy and server should be enabled where possible and configurable.
QueryableStateOptions,queryable-state.client.network-threads,Number of network (Netty's event loop) Threads for queryable state client.
StateBackendOptions,state.backend.type,"The state backend to be used to store state.<br />The implementation can be specified either via their shortcut  name, or via the class name of a <code class=""highlighter-rouge"">StateBackendFactory</code>. If a factory is specified it is instantiated via its zero argument constructor and its <code class=""highlighter-rouge"">StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader)</code> method is called.<br />Recognized shortcut names are 'hashmap' and 'rocksdb'."
StateBackendOptions,state.backend.latency-track.keyed-state-enabled,"Whether to track latency of keyed state operations, e.g value state put/get/clear."
StateBackendOptions,state.backend.latency-track.sample-interval,"The sample interval of latency track once 'state.backend.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests."
StateBackendOptions,state.backend.latency-track.history-size,Defines the number of measured latencies to maintain at each state access operation.
StateBackendOptions,state.backend.latency-track.state-name-as-variable,Whether to expose state name as a variable if tracking latency.
ConfigConstants,restart-strategy.fixed-delay.delay,
ConfigConstants,jobmanager.web.address,
ResourceManagerOptions,resourcemanager.job.timeout,Timeout for jobs which don't have a job manager as leader assigned.
ResourceManagerOptions,local.number-resourcemanager,The number of resource managers start.
ResourceManagerOptions,resourcemanager.rpc.port,"Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges."
ResourceManagerOptions,slotmanager.number-of-slots.min,"Defines the minimum number of slots that the Flink cluster allocates. This configuration option is meant for cluster to initialize certain workers in best efforts when starting. This can be used to speed up a job startup process. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink."
ResourceManagerOptions,slotmanager.number-of-slots.max,"Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink."
ResourceManagerOptions,slotmanager.min-total-resource.cpu,"Minimum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'."
ResourceManagerOptions,slotmanager.max-total-resource.cpu,"Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'."
ResourceManagerOptions,slotmanager.min-total-resource.memory,"Minimum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'."
ResourceManagerOptions,slotmanager.max-total-resource.memory,"Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'."
ResourceManagerOptions,slotmanager.redundant-taskmanager-num,"The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn).For fine-grained resource requirement, Redundant resources will be reserved, but it is possible that we have many small pieces of free resources form multiple TMs, which added up larger than the desired redundant resources, but each piece is too small to match the resource requirement of tasks from the failed worker."
ResourceManagerOptions,resourcemanager.start-worker.max-failure-rate,"The maximum number of start worker failures (Native Kubernetes / Yarn) per minute before pausing requesting new workers. Once the threshold is reached, subsequent worker requests will be postponed to after a configured retry interval ('resourcemanager.start-worker.retry-interval')."
ResourceManagerOptions,resourcemanager.start-worker.retry-interval,The time to wait before requesting new workers (Native Kubernetes / Yarn) once the max failure rate of starting workers ('resourcemanager.start-worker.max-failure-rate') is reached.
ResourceManagerOptions,slotmanager.requirement-check.delay,The delay of the resource requirements check.
ResourceManagerOptions,slotmanager.declare-needed-resource.delay,The delay of the declare needed resources.
ResourceManagerOptions,resourcemanager.standalone.start-up-time,"Time of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, <code class=""highlighter-rouge"">slot.request.timeout</code> will be used by default."
ResourceManagerOptions,slotmanager.taskmanager-timeout,The timeout for an idle task manager to be released.
ResourceManagerOptions,resourcemanager.taskmanager-timeout,The timeout for an idle task manager to be released.
ResourceManagerOptions,resourcemanager.taskmanager-release.wait.result.consumed,Release task executor only when each produced result partition is either consumed or failed. 'True' is default. 'False' means that idle task executor release is not blocked by receiver confirming consumption of result partition and can happen right away after 'resourcemanager.taskmanager-timeout' has elapsed. Setting this option to 'false' can speed up task executor release but can lead to unexpected failures if end of consumption is slower than 'resourcemanager.taskmanager-timeout'.
ResourceManagerOptions,resourcemanager.taskmanager-registration.timeout,"Timeout for TaskManagers to register at the active resource managers. If exceeded, active resource manager will release and try to re-request the resource for the worker. If not configured, fallback to 'taskmanager.registration.timeout'."
ResourceManagerOptions,resourcemanager.previous-worker.recovery.timeout,"Timeout for resource manager to recover all the previous attempts workers. If exceeded, resource manager will handle new resource requests by requesting new workers. If you would like to reuse the previous workers as much as possible, you should configure a longer timeout time to wait for previous workers to register."
PipelineOptionsInternal,$internal.pipeline.job-id,"**DO NOT USE** The static JobId to be used for the specific pipeline. For fault-tolerance, this value needs to stay the same across runs."
CoreOptions,classloader.resolve-order,"Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar (""child-first"") or the application classpath (""parent-first""). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively)."
CoreOptions,classloader.parent-first-patterns.default,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use ""classloader.parent-first-patterns.additional"" instead."
CoreOptions,classloader.parent-first-patterns.additional,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to ""classloader.parent-first-patterns.default""."
CoreOptions,classloader.fail-on-metaspace-oom-error,Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class.
CoreOptions,classloader.check-leaked-classloader,Fails attempts at loading classes if the user classloader of a job is used after it has terminated.
CoreOptions,plugin.classloader.parent-first-patterns.default,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the plugin parent ClassLoader first. A pattern is a simple prefix that is checked  against the fully qualified class name. This setting should generally not be modified. To add another  pattern we recommend to use ""plugin.classloader.parent-first-patterns.additional"" instead."
CoreOptions,plugin.classloader.parent-first-patterns.additional,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the plugin parent ClassLoader first. A pattern is a simple prefix that is checked  against the fully qualified class name. These patterns are appended to ""plugin.classloader.parent-first-patterns.default""."
CoreOptions,env.java.opts.all,Java options to start the JVM of all Flink processes with.
CoreOptions,env.java.opts.jobmanager,Java options to start the JVM of the JobManager with.
CoreOptions,env.java.opts.taskmanager,Java options to start the JVM of the TaskManager with.
CoreOptions,env.java.opts.historyserver,Java options to start the JVM of the HistoryServer with.
CoreOptions,env.java.opts.client,Java options to start the JVM of the Flink Client with.
CoreOptions,env.java.opts.sql-gateway,Java options to start the JVM of the Flink SQL Gateway with.
CoreOptions,env.java.default-opts.all,"A string of default JVM options to prepend to <code class=""highlighter-rouge"">env.java.opts.all</code>. This is intended to be set by administrators."
CoreOptions,env.java.default-opts.jobmanager,"A string of default JVM options to prepend to <code class=""highlighter-rouge"">env.java.opts.jobmanager</code>. This is intended to be set by administrators."
CoreOptions,env.java.default-opts.taskmanager,"A string of default JVM options to prepend to <code class=""highlighter-rouge"">env.java.opts.taskmanager</code>. This is intended to be set by administrators."
CoreOptions,env.log.dir,Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink’s home)
CoreOptions,env.pid.dir,Defines the directory where the flink-&lt;host&gt;-&lt;process&gt;.pid files are saved.
CoreOptions,env.log.max,The maximum number of old log files to keep.
CoreOptions,env.log.level,Defines the level of the root logger.
CoreOptions,env.stdout-err.redirect-to-file,"Whether redirect stdout and stderr to files when running foreground. If enabled, logs won't append the console too. Note that redirected files do not support rolling rotate."
CoreOptions,env.ssh.opts,"Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh)."
CoreOptions,env.hadoop.conf.dir,Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable.
CoreOptions,env.yarn.conf.dir,Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable.
CoreOptions,env.hbase.conf.dir,Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable.
CoreOptions,io.tmp.dirs,"Directories for temporary files, separated by"","", ""|"", or the system's java.io.File.pathSeparator."
CoreOptions,parallelism.default,Default parallelism for jobs.
CoreOptions,fs.default-scheme,"The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode."
CoreOptions,fs.allowed-fallback-filesystems,"A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb)"
CoreOptions,fs.overwrite-files,"Specifies whether file output writers should overwrite existing files by default. Set to ""true"" to overwrite by default,""false"" otherwise."
CoreOptions,fs.output.always-create-directory,"File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to ""true"", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to ""false"", the writer will directly create the file directly at the output path, without creating a containing directory."
TaskManagerOptions,taskmanager.heap.size,"JVM heap size for the TaskManagers, which are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager's YARN container, minus a certain tolerance value."
TaskManagerOptions,taskmanager.heap.mb,"JVM heap size (in megabytes) for the TaskManagers, which are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager's YARN container, minus a certain tolerance value."
TaskManagerOptions,taskmanager.jvm-exit-on-oom,Whether to kill the TaskManager when the task thread throws an OutOfMemoryError.
TaskManagerOptions,taskmanager.exit-on-fatal-akka-error,Whether the quarantine monitor for task managers shall be started. The quarantine monitor shuts down the actor system if it detects that it has quarantined another actor system or if it has been quarantined by another actor system.
TaskManagerOptions,taskmanager.host,"The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file."
TaskManagerOptions,taskmanager.bind-host,"The local address of the network interface that the task manager binds to. If not configured, '0.0.0.0' will be used."
TaskManagerOptions,taskmanager.rpc.port,"The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine."
TaskManagerOptions,taskmanager.rpc.bind-port,"The local RPC port that the TaskManager binds to. If not configured, the external port (configured by 'taskmanager.rpc.port') will be used."
TaskManagerOptions,taskmanager.collect-sink.port,"The port used for the client to retrieve query results from the TaskManager. The default value is 0, which corresponds to a random port assignment."
TaskManagerOptions,taskmanager.registration.initial-backoff,The initial registration backoff between two consecutive registration attempts. The backoff is doubled for each new registration attempt until it reaches the maximum registration backoff.
TaskManagerOptions,taskmanager.registration.max-backoff,The maximum registration backoff between two consecutive registration attempts. The max registration backoff requires a time unit specifier (ms/s/min/h/d).
TaskManagerOptions,taskmanager.registration.refused-backoff,The backoff after a registration has been refused by the job manager before retrying to connect.
TaskManagerOptions,taskmanager.registration.timeout,"Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates."
TaskManagerOptions,taskmanager.numberOfTaskSlots,"The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores)."
TaskManagerOptions,taskmanager.slot.timeout,"Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no value is configured, then it will fall back to <code class=""highlighter-rouge"">pekko.ask.timeout</code>."
TaskManagerOptions,taskmanager.debug.memory.log,"Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM."
TaskManagerOptions,taskmanager.debug.memory.log-interval,The interval for the log thread to log the current memory usage.
TaskManagerOptions,taskmanager.memory.segment-size,Size of memory buffers used by the network stack and the memory manager.
TaskManagerOptions,taskmanager.memory.min-segment-size,Minimum possible size of memory buffers used by the network stack and the memory manager. ex. can be used for automatic buffer size adjustment.
TaskManagerOptions,taskmanager.network.bind-policy,"The automatic address binding policy used by the TaskManager if ""taskmanager.host"" is not set. The value should be one of the following:"
TaskManagerOptions,taskmanager.resource-id,"The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the ""RpcAddress:RpcPort"" and a 6-character random string. Notice that this option is not valid in Yarn and Native Kubernetes mode."
TaskManagerOptions,taskmanager.cpu.cores,"CPU cores for the TaskExecutors. In case of Yarn setups, this value will be rounded to the closest positive integer. If not explicitly configured, legacy config options 'yarn.containers.vcores' and 'kubernetes.taskmanager.cpu' will be used for Yarn / Kubernetes setups, and 'taskmanager.numberOfTaskSlots' will be used for standalone setups (approximate number of slots)."
TaskManagerOptions,taskmanager.memory.process.size,"Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration."
TaskManagerOptions,taskmanager.memory.flink.size,"Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration."
TaskManagerOptions,taskmanager.memory.framework.heap.size,"Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots."
TaskManagerOptions,taskmanager.memory.framework.off-heap.size,"Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter."
TaskManagerOptions,taskmanager.memory.task.heap.size,"Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory."
TaskManagerOptions,taskmanager.memory.task.off-heap.size,Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.
TaskManagerOptions,taskmanager.memory.managed.size,"Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory."
TaskManagerOptions,taskmanager.memory.managed.fraction,"Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified."
TaskManagerOptions,taskmanager.memory.managed.consumer-weights,"Managed memory weights for different kinds of consumers. A slot’s managed memory is shared by all kinds of consumers it contains, proportionally to the kinds’ weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are OPERATOR (for built-in algorithms), STATE_BACKEND (for RocksDB state backend) and PYTHON (for Python processes)."
TaskManagerOptions,taskmanager.memory.network.min,"Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value."
TaskManagerOptions,taskmanager.memory.network.max,"Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. By default, the max limit of Network Memory is Long.MAX_VALUE. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value."
TaskManagerOptions,taskmanager.memory.network.fraction,"Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value."
TaskManagerOptions,taskmanager.network.memory.buffer-debloat.period,The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance.
TaskManagerOptions,taskmanager.network.memory.buffer-debloat.samples,The number of the last buffer size values that will be taken for the correct calculation of the new one.
TaskManagerOptions,taskmanager.network.memory.buffer-debloat.target,"The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data."
TaskManagerOptions,taskmanager.network.memory.buffer-debloat.enabled,The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput.
TaskManagerOptions,taskmanager.network.memory.buffer-debloat.threshold-percentages,The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments.
TaskManagerOptions,taskmanager.memory.framework.off-heap.batch-shuffle.size,"Size of memory used by batch shuffle for shuffle data read (currently only used by sort-shuffle and hybrid shuffle). Notes: 1) The memory is cut from 'taskmanager.memory.framework.off-heap.size' so must be smaller than that, which means you may also need to increase 'taskmanager.memory.framework.off-heap.size' after you increase this config value; 2) This memory size can influence the shuffle performance and you can increase this config value for large-scale batch jobs (for example, to 128M or 256M)."
TaskManagerOptions,taskmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the TaskExecutors.
TaskManagerOptions,taskmanager.memory.jvm-overhead.min,"Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value."
TaskManagerOptions,taskmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value."
TaskManagerOptions,taskmanager.memory.jvm-overhead.fraction,"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value."
TaskManagerOptions,task.cancellation.interval,Time interval between two successive task cancellation attempts.
TaskManagerOptions,task.cancellation.timeout,Timeout after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog. Notice that a task cancellation is different from both a task failure and a clean shutdown.  Task cancellation timeout only applies to task cancellation and does not apply to task closing/clean-up caused by a task failure or a clean shutdown.
TaskManagerOptions,task.cancellation.timers.timeout,Time we wait for the timers to finish all pending timer threads when the stream task is cancelled.
TaskManagerOptions,taskmanager.system-out.mode,"Redirection mode of <code class=""highlighter-rouge"">System.out</code> and <code class=""highlighter-rouge"">System.err</code> for all <code class=""highlighter-rouge"">TaskManagers</code>.<ul><li><code class=""highlighter-rouge"">DEFAULT</code>: <code class=""highlighter-rouge"">TaskManagers</code> don't redirect the <code class=""highlighter-rouge"">System.out</code> and <code class=""highlighter-rouge"">System.err</code>, it's the default value.</li><li><code class=""highlighter-rouge"">LOG</code>: <code class=""highlighter-rouge"">TaskManagers</code> redirect <code class=""highlighter-rouge"">System.out</code> and <code class=""highlighter-rouge"">System.err</code> to LOG.info and LOG.error.</li><li><code class=""highlighter-rouge"">IGNORE</code>: <code class=""highlighter-rouge"">TaskManagers</code> ignore <code class=""highlighter-rouge"">System.out</code> and <code class=""highlighter-rouge"">System.err</code> directly.</li></ul>"
TaskManagerOptions,taskmanager.system-out.log.thread-name.enabled,"Whether to log the thread name when <code class=""highlighter-rouge"">taskmanager.system-out.mode</code> is LOG."
TaskManagerOptions,taskmanager.system-out.log.cache-upper-size,"The cache upper size when Flink caches current line context of <code class=""highlighter-rouge"">System.out</code> or <code class=""highlighter-rouge"">System.err</code> when <code class=""highlighter-rouge"">taskmanager.system-out.mode</code> is LOG."
TaskManagerOptions,taskmanager.load-balance.mode,"Mode for the load-balance allocation strategy across all available <code class=""highlighter-rouge"">TaskManagers</code>.<ul><li>The <code class=""highlighter-rouge"">SLOTS</code> mode tries to spread out the slots evenly across all available <code class=""highlighter-rouge"">TaskManagers</code>.</li><li>The <code class=""highlighter-rouge"">NONE</code> mode is the default mode without any specified strategy.</li></ul>"
TaskManagerOptions,taskmanager.log.path,The path to the log file of the task manager.
TaskManagerOptions,taskmanager.runtime.fs-timeout,The timeout for filesystem stream opening. A value of 0 indicates infinite waiting.
TaskManagerOptions,minicluster.number-of-taskmanagers,The number of task managers of MiniCluster.
StateRecoveryOptions,execution.state-recovery.path,Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537).
StateRecoveryOptions,execution.state-recovery.ignore-unclaimed-state,Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered.
StateRecoveryOptions,execution.state-recovery.claim-mode,Describes the mode how Flink should restore from the given savepoint or retained checkpoint.
StateRecoveryOptions,execution.state-recovery.approximate-local-recovery,Flag to enable approximate local recovery.
StateRecoveryOptions,execution.state-recovery.without-channel-state.checkpoint-id,Checkpoint id for which in-flight data should be ignored in case of the recovery from this checkpoint.<br /><br />It is better to keep this value empty until there is explicit needs to restore from the specific checkpoint without in-flight data.<br />
StateRecoveryOptions,execution.state-recovery.from-local,"This option configures local recovery for the state backend, which indicates whether to recovery from local snapshot.By default, local recovery is deactivated. Local recovery currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend)."""
RpcOptions,pekko.rpc.force-invocation-serialization,"Forces the serialization of all RPC invocations (that are not explicitly annotated with <code class=""highlighter-rouge"">org.apache.flink.runtime.rpc.Local</code>).This option can be used to find serialization issues in the argument/response types without relying requiring HA setups.This option should not be enabled in production."
RpcOptions,pekko.ask.callstack,"If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint."
RpcOptions,pekko.ask.timeout,Timeout used for all futures and blocking Pekko calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d).
RpcOptions,pekko.tcp.timeout,"Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value."
RpcOptions,pekko.startup-timeout,Timeout after which the startup of a remote component is considered being failed.
RpcOptions,pekko.ssl.enabled,Turns on SSL for Pekko’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true.
RpcOptions,pekko.framesize,"Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier."
RpcOptions,pekko.throughput,Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness.
RpcOptions,pekko.log.lifecycle.events,Turns on the Pekko’s remote logging of events. Set this value to 'true' in case of debugging.
RpcOptions,pekko.lookup.timeout,Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).
RpcOptions,pekko.jvm-exit-on-fatal-error,Exit JVM on fatal Pekko errors.
RpcOptions,pekko.retry-gate-closed-for,Milliseconds a gate should be closed for after a remote connection was disconnected.
RpcOptions,pekko.fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.
RpcOptions,pekko.fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.
RpcOptions,pekko.fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.
RpcOptions,pekko.remote-fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.
RpcOptions,pekko.remote-fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.
RpcOptions,pekko.remote-fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.
RpcOptions,pekko.client-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.
RpcOptions,pekko.client-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.
RpcOptions,pekko.client-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
RpcOptions,pekko.server-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.
RpcOptions,pekko.server-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.
RpcOptions,pekko.server-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
CheckpointingOptions,state.backend.type,"The state backend to be used to store state.<br />The implementation can be specified either via their shortcut  name, or via the class name of a <code class=""highlighter-rouge"">StateBackendFactory</code>. If a factory is specified it is instantiated via its zero argument constructor and its <code class=""highlighter-rouge"">StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader)</code> method is called.<br />Recognized shortcut names are 'hashmap' and 'rocksdb'."
CheckpointingOptions,execution.checkpointing.storage,"The checkpoint storage implementation to be used to checkpoint state.<br />The implementation can be specified either via their shortcut  name, or via the class name of a <code class=""highlighter-rouge"">CheckpointStorageFactory</code>. If a factory is specified it is instantiated via its zero argument constructor and its <code class=""highlighter-rouge"">CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader)</code>  method is called.<br />Recognized shortcut names are 'jobmanager' and 'filesystem'.<br />'execution.checkpointing.storage' and 'execution.checkpointing.dir' are usually combined to configure the checkpoint location. By default,  the checkpoint meta data and actual program state will be stored in the JobManager's memory directly. When 'execution.checkpointing.storage' is set to 'jobmanager', if 'execution.checkpointing.dir' is configured, the meta data of checkpoints will be persisted to the path specified by 'execution.checkpointing.dir'. Otherwise, the meta data will be stored in the JobManager's memory. When 'execution.checkpointing.storage' is set to 'filesystem', a valid path must be configured to 'execution.checkpointing.dir', and the checkpoint meta data and actual program state will both be persisted to the path."
CheckpointingOptions,execution.checkpointing.num-retained,The maximum number of completed checkpoints to retain.
CheckpointingOptions,execution.checkpointing.cleaner.parallel-mode,Option whether to discard a checkpoint's states in parallel using the ExecutorService passed into the cleaner
CheckpointingOptions,state.backend.async,Deprecated option. All state snapshots are asynchronous.
CheckpointingOptions,execution.checkpointing.incremental,"Option whether to create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option."
CheckpointingOptions,state.backend.local-recovery,"This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend)."
CheckpointingOptions,execution.checkpointing.local-backup.dirs,"The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. If not configured it will default to &lt;WORKING_DIR&gt;/localState. The &lt;WORKING_DIR&gt; can be configured via <code class=""highlighter-rouge"">process.taskmanager.working-dir</code>"
CheckpointingOptions,execution.checkpointing.savepoint-dir,"The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend)."
CheckpointingOptions,execution.checkpointing.dir,"The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). If the 'execution.checkpointing.storage' is set to 'jobmanager', only the meta data of checkpoints will be stored in this directory."
CheckpointingOptions,execution.checkpointing.create-subdir,"Whether to create sub-directories named by job id under the '<code class=""highlighter-rouge"">execution.checkpointing.dir</code>' to store the data files and meta data of checkpoints. The default value is true to enable user could run several jobs with the same checkpoint directory at the same time. If this value is set to false, pay attention not to run several jobs with the same directory simultaneously. <br />WARNING: This is an advanced configuration. If set to false, users must ensure that no multiple jobs are run with the same checkpoint directory, and that no files exist other than those necessary for the restoration of the current job when starting a new job."
CheckpointingOptions,execution.checkpointing.data-inline-threshold,The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB.
CheckpointingOptions,execution.checkpointing.write-buffer-size,The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'execution.checkpointing.data-inline-threshold'.
CheckpointingOptions,execution.checkpointing.local-backup.enabled,"This option configures local backup for the state backend, which indicates whether to make backup checkpoint on local disk.  If not configured, fallback to execution.state-recovery.from-local. By default, local backup is deactivated. Local backup currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend)."
CheckpointingOptions,execution.checkpointing.file-merging.enabled,"Whether to enable merging multiple checkpoint files into one, which will greatly reduce the number of small checkpoint files. This is an experimental feature under evaluation, make sure you're aware of the possible effects of enabling it."
CheckpointingOptions,execution.checkpointing.file-merging.across-checkpoint-boundary,"Only relevant if <code class=""highlighter-rouge"">execution.checkpointing.file-merging.enabled</code> is enabled.<br />Whether to allow merging data of multiple checkpoints into one physical file. If this option is set to false, only merge files within checkpoint boundaries. Otherwise, it is possible for the logical files of different checkpoints to share the same physical file."
CheckpointingOptions,execution.checkpointing.file-merging.max-file-size,Max size of a physical file for merged checkpoints.
CheckpointingOptions,execution.checkpointing.file-merging.pool-blocking,"Whether to use Blocking or Non-Blocking pool for merging physical files. A Non-Blocking pool will always provide usable physical file without blocking. It may create many physical files if poll file frequently. When poll a small file from a Blocking pool, it may be blocked until the file is returned."
CheckpointingOptions,execution.checkpointing.file-merging.max-subtasks-per-file,The upper limit of the file pool size based on the number of subtasks within each TM(only for merging private state at Task Manager level).
CheckpointingOptions,execution.checkpointing.file-merging.max-space-amplification,"Space amplification stands for the magnification of the occupied space compared to the amount of valid data. The more space amplification is, the more waste of space will be. This configs a space amplification above which a re-uploading for physical files will be triggered to reclaim space. Any value below 1f means disabling the space control."
CheckpointingOptions,execution.checkpointing.mode,The checkpointing mode (exactly-once vs. at-least-once).
CheckpointingOptions,execution.checkpointing.timeout,The maximum time that a checkpoint may take before being discarded.
CheckpointingOptions,execution.checkpointing.max-concurrent-checkpoints,"The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire."
CheckpointingOptions,execution.checkpointing.min-pause,"The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see <code class=""highlighter-rouge"">execution.checkpointing.max-concurrent-checkpoints</code>).<br /><br />If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all."
CheckpointingOptions,execution.checkpointing.tolerable-failed-checkpoints,"The tolerable checkpoint consecutive failure number. If set to 0, that means we do not tolerance any checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored."
CheckpointingOptions,execution.checkpointing.externalized-checkpoint-retention,"Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status <code class=""highlighter-rouge"">JobStatus#FAILED</code> or <code class=""highlighter-rouge"">JobStatus#SUSPENDED</code>). In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state.<br /><br />The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status <code class=""highlighter-rouge"">JobStatus#CANCELED</code>).<br /><br />The target directory for externalized checkpoints is configured via <code class=""highlighter-rouge"">execution.checkpointing.dir</code>."
CheckpointingOptions,execution.checkpointing.interval-during-backlog,"If it is not null and any source reports isProcessingBacklog=true, it is the interval in which checkpoints are periodically scheduled.<br /><br />Checkpoint triggering may be delayed by the settings <code class=""highlighter-rouge"">execution.checkpointing.max-concurrent-checkpoints</code> and <code class=""highlighter-rouge"">execution.checkpointing.min-pause</code>.<br /><br />Note: if it is not null, the value must either be 0, which means the checkpoint is disabled during backlog, or be larger than or equal to execution.checkpointing.interval."
CheckpointingOptions,execution.checkpointing.interval,"Gets the interval in which checkpoints are periodically scheduled.<br /><br />This setting defines the base interval. Checkpoint triggering may be delayed by the settings <code class=""highlighter-rouge"">execution.checkpointing.max-concurrent-checkpoints</code>, <code class=""highlighter-rouge"">execution.checkpointing.min-pause</code> and <code class=""highlighter-rouge"">execution.checkpointing.interval-during-backlog</code>"
CheckpointingOptions,execution.checkpointing.unaligned.enabled,"Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure.<br /><br />Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.<br /><br />Unaligned checkpoints can only be enabled if <code class=""highlighter-rouge"">execution.checkpointing.mode</code> is <code class=""highlighter-rouge"">EXACTLY_ONCE</code> and if <code class=""highlighter-rouge"">execution.checkpointing.max-concurrent-checkpoints</code> is 1"
CheckpointingOptions,execution.checkpointing.aligned-checkpoint-timeout,"Only relevant if <code class=""highlighter-rouge"">execution.checkpointing.unaligned.enabled</code> is enabled.<br /><br />If timeout is 0, checkpoints will always start unaligned.<br /><br />If timeout has a positive value, checkpoints will start aligned. If during checkpointing, checkpoint start delay exceeds this timeout, alignment will timeout and checkpoint barrier will start working as unaligned checkpoint."
CheckpointingOptions,execution.checkpointing.unaligned.forced,"Forces unaligned checkpoints, particularly allowing them for iterative jobs."
CheckpointingOptions,execution.checkpointing.unaligned.interruptible-timers.enabled,"Allows unaligned checkpoints to skip timers that are currently being fired. For this feature to be enabled, it must be also supported by the operator. Currently this is supported by all TableStreamOperators and CepOperator."
CheckpointingOptions,execution.checkpointing.checkpoints-after-tasks-finish,"Feature toggle for enabling checkpointing even if some of tasks have finished. Before you enable it, please take a look at <a href=""{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/dev/datastream/fault-tolerance/checkpointing/#checkpointing-with-parts-of-the-graph-finished-beta"">the important considerations</a>"
CheckpointingOptions,execution.checkpointing.unaligned.max-subtasks-per-channel-state-file,Defines the maximum number of subtasks that share the same channel state file. It can reduce the number of small files when enable unaligned checkpoint. Each subtask will create a new channel state file when this is configured to 1.
HistoryServerOptions,historyserver.archive.fs.refresh-interval,Interval for refreshing the archived job directories.
HistoryServerOptions,historyserver.archive.fs.dir,Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via `jobmanager.archive.fs.dir`.
HistoryServerOptions,historyserver.archive.clean-expired-jobs,Whether HistoryServer should cleanup jobs that are no longer present `historyserver.archive.fs.dir`.
HistoryServerOptions,historyserver.log.taskmanager.url-pattern,"Pattern of the log URL of TaskManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, `&lt;jobid&gt;` and `&lt;tmid&gt;`, to the id of job and TaskManager respectively. Only http / https schemes are supported."
HistoryServerOptions,historyserver.log.jobmanager.url-pattern,"Pattern of the log URL of JobManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, `&lt;jobid&gt;`, to the id of job. Only http / https schemes are supported."
HistoryServerOptions,historyserver.web.tmpdir,Local directory that is used by the history server REST API for temporary files.
HistoryServerOptions,historyserver.web.address,Address of the HistoryServer's web interface.
HistoryServerOptions,historyserver.web.port,Port of the HistoryServers's web interface.
HistoryServerOptions,historyserver.web.refresh-interval,The refresh interval for the HistoryServer web-frontend.
HistoryServerOptions,historyserver.web.ssl.enabled,Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true.
HistoryServerOptions,historyserver.archive.retained-jobs,"The maximum number of jobs to retain in each archive directory defined by `historyserver.archive.fs.dir`. If set to `-1`(default), there is no limit to the number of archives. If set to `0` or less than `-1` HistoryServer will throw an <code class=""highlighter-rouge"">IllegalConfigurationException</code>."
ClusterOptions,cluster.registration.initial-timeout,Initial registration timeout between cluster components.
ClusterOptions,cluster.registration.max-timeout,Maximum registration timeout between cluster components.
ClusterOptions,cluster.registration.error-delay,The pause made after an registration attempt caused an exception (other than timeout).
ClusterOptions,cluster.registration.refused-registration-delay,The pause made after the registration attempt was refused.
ClusterOptions,cluster.services.shutdown-timeout,The shutdown timeout for cluster services like executors.
ClusterOptions,cluster.io-pool.size,The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently.
ClusterOptions,cluster.evenly-spread-out-slots,"Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available <code class=""highlighter-rouge"">TaskExecutors</code>."
ClusterOptions,cluster.processes.halt-on-fatal-error,"Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See <a href=""https://issues.apache.org/jira/browse/FLINK-16510"">FLINK-16510</a> for details."
ClusterOptions,cluster.intercept-user-system-exit,"Flag to check user code exiting system by terminating JVM (e.g., System.exit()). Note that this configuration option can interfere with <code class=""highlighter-rouge"">cluster.processes.halt-on-fatal-error</code>: In intercepted user-code, a call to System.exit() will not cause the JVM to halt, when <code class=""highlighter-rouge"">THROW</code> is configured."
ClusterOptions,cluster.thread-dump.stacktrace-max-depth,The maximum stacktrace depth of TaskManager and JobManager's thread dump web-frontend displayed.
ClusterOptions,fine-grained.shuffle-mode.all-blocking,Whether to convert all PIPELINE edges to BLOCKING when apply fine-grained resource management in batch jobs.
ClusterOptions,cluster.uncaught-exception-handling,"Defines whether cluster will handle any uncaught exceptions by just logging them (LOG mode), or by failing job (FAIL mode)"
ClusterOptions,process.working-dir,"Local working directory for Flink processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to a randomly picked temporary directory defined via <code class=""highlighter-rouge"">io.tmp.dirs</code>."
ClusterOptions,process.jobmanager.working-dir,"Working directory for Flink JobManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to <code class=""highlighter-rouge"">process.working-dir</code>."
ClusterOptions,process.taskmanager.working-dir,"Working directory for Flink TaskManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to <code class=""highlighter-rouge"">process.working-dir</code>."
TaskManagerOptionsInternal,internal.taskmanager.resource-id.metadata,**DO NOT USE** The metadata of TaskManager's ResourceID to be used for logging.
TaskManagerOptionsInternal,internal.taskmanager.node-id,ID of the node where the TaskManager is located on.
